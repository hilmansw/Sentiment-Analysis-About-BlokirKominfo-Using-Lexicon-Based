# -*- coding: utf-8 -*-
"""HSW - Sentiment Analysis About #BlokirKominfo Using Lexion Based.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SYOfr_OSTFK81bIDHLj38srpRTSnGaCr

# **Sentiment Analysis About #BlokirKominfo Using Lexion Based**
### **Created by:** 
**Hilman Singgih Wicaksana, S.Kom**<br>
Masters Student of Information Systems at Diponegoro University, Semarang.

# **Crawling Data**
"""

!pip install tweepy

import tweepy
import pandas as pd
import csv

access_token = "[your_access_token]"
access_secret = "[your_access_secret_token]"
consumer_key = "[your_consumer_key_token]"
consumer_secret = "[your_consumer_secret_token]"

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)
api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=False, compression=True)

def scraptweets(search_words, date_since, date_until):
  db_tweets = pd.DataFrame(columns=['username', 'tweetcreatedts', 'text'])
  tweets = tweepy.Cursor(
      api.search, q=search_words, lang="id",
      since=date_since, until=date_until, tweet_mode="extended").items(1000)
  tweet_list = [tweet for tweet in tweets]

  for tweet in tweet_list:
    username = tweet.user.screen_name
    tweetcreatedts = tweet.created_at

    try:
      text = tweet.retweeted.status.full_text
    except AttributeError:
      text = tweet.full_text

    ith_tweet = [username, tweetcreatedts, text]

    db_tweets.loc[len(db_tweets)] = ith_tweet

    print("Proses scrapping selesai dengan jumlah data", len(db_tweets))

  filename = "kominfo_sentiment.csv"
  db_tweets.to_csv(filename, index=False)

search_words = "#BlokirKominfo"
date_since = "2022-07-01"
date_until = "2022-07-31"

scraptweets(search_words, date_since, date_until)

#Load data
import pandas as pd
def load_data():
  data = pd.read_csv("kominfo_sentiment.csv")
  return data

tweet_df = load_data()

#Buka tabel tweet
tweet_df = pd.DataFrame(tweet_df[['username', 'tweetcreatedts', 'text']])
tweet_df.head(10)

"""# **Preprocessing**

## **Cleaning**
"""

import numpy as np
import re

#remove mention username dan retweet
def remove_pattern(text, pattern_regex):
  r = re.findall(pattern_regex, text)
  for i in r:
    text = re.sub(i, '', text)
  return text

tweet_df['clean_tweet'] = np.vectorize(remove_pattern)(tweet_df['text'], " *RT* | *@[\w:]*")
tweet_df.head()

#remove simbol
def remove(text):
  text = ' ' .join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text).split())
  return text

tweet_df['remove_http'] = tweet_df['clean_tweet'].apply(lambda x: remove(x))
tweet_df.head()

#remove hashtag
def remov(tweet):
  tweet = re.sub(r'\$\w*', '', tweet)
  tweet = re.sub(r'^RT[\s]+', '', tweet)

  tweet = re.sub(r'#', '', tweet)
  tweet = re.sub('[0-9]+', '', tweet)

  return tweet

tweet_df['remove_hashtag'] = tweet_df['remove_http'].apply(lambda x: remov(x))
tweet_df.head()

#Remove data duplikat
tweet_df.drop_duplicates(subset="remove_hashtag", keep="first", inplace=True)
tweet_df.head()

"""# **Case Folding**

Membuat huruf dari tweet menjadi ukuran kecil, agar komputer lebih mudah dalam membaca data tersebut

* Tokenizing = memisahkan teks tweet menjadi potongan per kata
* Filtering = menghilangkan kata-kata yang kurang bermakna (seperti: yang, untuk, dsb)
* Stemming = menjadikan kata dasar (misal: mencintaimu = cinta)
"""

!pip install Sastrawi

#import stopword
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stopwords_indonesia = stopwords.words('indonesian')

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary

stop_factory = StopWordRemoverFactory().get_stop_words()
more_stopwords = [
    'yg', 'kpd', 'utk', 'cuman', 'deh', 'Btw', 'tapi', 'gua', 'gue', 'lo', 'lu',
    'kalo', 'trs', 'jd', 'nih', 'ntar', 'nya', 'lg', 'gk', 'dpt', 'dr', 'kpn',
    'kok', 'kyk', 'donk', 'yah', 'u', 'ya', 'ga', 'gak', 'km', 'eh', 'sih',
    'bang', 'bro', 'sob', 'mas', 'mba', 'haha', 'wkwk', 'kmrn', 'iy', 'affa',
    'iyah', 'lho', 'sbnry', 'tuh', 'kzl', 'hahaha', 'weh', 'tuh'
] 

data = stop_factory + more_stopwords

dictionary = ArrayDictionary(data)
str = StopWordRemover(dictionary)

print(data)

#import sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

#tokenize
import string
from nltk.tokenize import TweetTokenizer

happy_emoticons = set([
    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':x', ':>', '=]', '8)',
    ':-D', ':D', ':^)', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D',
    '=D', '=-3', ':-))', ':-)', "-')", ':*', ':^*', '>:P', ':-P', ':P',
    'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)',
    '>:-)', '<3'
])

sad_emoticons = set([
    ':L', ':-/', '>:/', ':$', '>:[', ':@', ':-(', ':[', ':-||', '=L',
    ':<', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'(", ':\\', ':-c',
    ':c', ':(', '>:\\', ':('
])

all_emoticons = happy_emoticons.union(sad_emoticons)

def clean_tweets(tweet):
  #tokenize tweets
  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                             reduce_len=True)
  tweet_tokens = tokenizer.tokenize(tweet)

  tweet_clean = []

  for word in tweet_tokens:
    if(word not in data and
       word not in all_emoticons and
       word not in string.punctuation):
      stem_word = stemmer.stem(word)
      tweet_clean.append(stem_word) 
  
  return tweet_clean

tweet_df['Tweet'] = tweet_df['remove_hashtag'].apply(lambda x: clean_tweets(x))
tweet_df.head()

#remove punct
def remove_punct(text):
  text = " ".join([char for char in text if char not in string.punctuation])
  return text

tweet_df['Tweet'] = tweet_df['Tweet'].apply(lambda x: remove_punct(x))
tweet_df.head()

#remove tweet kosong
tweet_df = tweet_df[tweet_df['Tweet'] != '']
tweet_df.head()

#reset_index
tweet_df = tweet_df.reset_index(drop=True)
tweet_df.head()

tweet_df.drop_duplicates(subset="remove_hashtag", keep="first", inplace=True)
tweet_df.head(10)

#remove kolom
tweet_df.drop(tweet_df.columns[[0, 1, 2, 3, 4, 5]], axis=1, inplace=True)
tweet_df.head()

#simpan data bersih
tweet_df.to_csv('clean_kominfo_sentiment.csv', encoding='utf8', index=False)

#install googletrans
!pip install googletrans==3.1.0a0

import pandas as pd 
import googletrans
from googletrans import Translator

df = pd.read_csv("clean_kominfo_sentiment.csv")
df.head()

translator = Translator()
translations = {}
for column in df.columns:
  unique_elements = df[column].unique()
  for element in unique_elements:
    translations[element] = translator.translate(element).text
translations

#mengganti semua kata yang diterjemahkan dari kamus ke original dataframe
df.replace(translations, inplace=True)
df.head(10)

#simpan hasil translasi
df.to_csv('translation_kominfo_sentiment.csv', encoding='utf8', index=False)

import pandas as pd
def load_data():
  data = pd.read_csv('translation_kominfo_sentiment.csv')
  return data

tweet_df = load_data()
tweet_df.head()

"""# **Classification using Lexicon Based method**"""

!pip install VaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyser = SentimentIntensityAnalyzer()

scores = [analyser.polarity_scores(x) for x in tweet_df['Tweet']]
scores

tweet_df['Compound_Score'] = [x['compound'] for x in scores]
tweet_df.head()

#compound score lexicon based
tweet_df.loc[tweet_df['Compound_Score'] < 0, 'Sentiments'] = 'Negatif'
tweet_df.loc[tweet_df['Compound_Score'] == 0, 'Sentiments'] = 'Netral'
tweet_df.loc[tweet_df['Compound_Score'] > 0, 'Sentiments'] = 'Positif'
tweet_df.head()

#simpan hasil klasifikasi
tweet_df.to_csv('classification_kominfo_sentiment.csv', encoding='utf8', index=False)

"""# **Visualization**"""

!pip install tweet-preprocessor
!pip install textblob
!pip install emoji
!pip install wordcloud

def load_data():
  data = pd.read_csv('classification_kominfo_sentiment.csv')
  return data

tweet_df = load_data()
tweet_df.head()

!pip install matplotlib==3.1.0

import matplotlib.pyplot as plt
import pandas as pd

fig, ax = plt.subplots(figsize = (10, 10))
sizes = [count for count in tweet_df['Sentiments'].value_counts()]
labels = list(tweet_df['Sentiments'].value_counts().index)
explode = (0.1, 0, 0)
ax.pie(x = sizes, labels = labels, autopct = '%1.1f%%', explode = explode, textprops={'fontsize': 16})
ax.set_title('Polarity Percentage of Sentiment\nAbout #BlokirKominfo on Twitter', fontsize = 20, pad = 30)
fig.set_facecolor('white')
plt.show()
plt.savefig('pie_chart.png')

s = pd.value_counts(tweet_df['Sentiments'])
ax = s.plot.bar(figsize = (10, 9))
plt.rcParams['axes.facecolor'] = 'white'

for p in ax.patches:
  ax.annotate("{:,}".format(int(p.get_height())), (p.get_x() * 1.005, p.get_height() * 1.005), fontsize = 14)

plt.show()
plt.savefig('bar_chart.png')

#sentiment negatif
negative_sentiment = (tweet_df['Sentiments'] == 'Negatif').sum()
print("The number of negative sentiments is:\t", negative_sentiment)

#sentiment positif
positive_sentiment = (tweet_df['Sentiments'] == 'Positif').sum()
print("The number of positive sentiments is:\t", positive_sentiment)

#sentiment netral
neutral_sentiment = (tweet_df['Sentiments'] == 'Netral').sum()
print("The number of neutral sentiments is:\t", neutral_sentiment)

# total
total = len(tweet_df.index)
print("The overall total of sentiments is:\t", total)

from wordcloud import WordCloud

#Creating the text variable
text = " ".join(tweet for tweet in tweet_df['Tweet'])

# Generate word cloud
word_cloud = WordCloud(width=1600, height=800, max_font_size=200,
                       background_color='black').generate(text)

# Display the generated Word Cloud
plt.figure(figsize=(12,10))
plt.imshow(word_cloud)
plt.axis("off")
plt.show()
plt.savefig('wordcloud.png')
